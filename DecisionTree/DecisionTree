import sys
import numpy
import pandas

class DecisionTree:
  def __init__(self, feature):
    self.feature =  feature
    self.children = [] #maybe  a dict?
  
  def add_child(self, child):
    self.children.append(child)


def ID3(S, Attributes, Label):
  """
  construncts the desicion tree
  S is the set of examples
  Label is the target label ??
  Attributes is the set of measurred attributes
  """
  # 1. Create  Root Node
  # 2. A = Attribute that best splits S
  # NOTE: do steps 2 then 1. 
  # 3. for each v that A can take
    # a. add new branch to the  tree  A=v
    # let  S_v be  subset S where  A = v
    # c. if S_v is empty  
      # add leaf node with most common value of label
    # d. else
      #add to subtree ID3(S_v, Attributes -  {A}, Label)
      
  #  4. return root
  
# functions for splitting given to the Decision tree
def information_gain(df, attribute):
  total_entropy, total_values = entropy(df)
  allAttributes= df.groupby(attribute)[attribute].count()
  total_attribute_entropy = 0
  for index, row in allAttributes.items():
    # print(index)
    # print(row)
    print(df.loc[df[attribute] == index])
    
    attribute_entropy, count = entropy(df.loc[df[attribute] == index])
    total_attribute_entropy += (count / total_values) * attribute_entropy
  return (total_entropy - total_attribute_entropy)

def entropy(df):
  """
  returns the entropy of a set
  I think this is ok works on test1
  """
  set_entropy = 0
  allLabels = df.groupby("label")["label"].count()
  total  = allLabels.sum()
  
  for index, row in allLabels.items():
    probOfLabel = row/total
    set_entropy -= probOfLabel * numpy.log2(probOfLabel)
  return set_entropy, total

def gini_index():
  return 0

def majority_error():
  return 0

def get_training_data(file_path):
  #TODO make headers generic
  # train_df = pandas.read_csv(file_path, names=["buying", "maint", "doors", "persons", "lug_boot", "safety", "label"])
  train_df = pandas.read_csv(file_path)
  # print("read the data")
  return train_df

if __name__ == "__main__":
  # argv will likely  look  like Train.csv split_function Test.csv
  if len(sys.argv) >= 2:
    train_df =  get_training_data(sys.argv[1])
    # print(train_df)
    e, c = entropy(train_df)
    print(e)
    print("x1")
    ig = information_gain(train_df, "x1")
    print(ig)
    print("x2")
    ig = information_gain(train_df, "x2")
    print(ig)
    print("x3")
    ig = information_gain(train_df, "x3")
    print(ig)
    print("x4")
    ig = information_gain(train_df, "x4")
    print(ig)
  else: 
    print("no data given")