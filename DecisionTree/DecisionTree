import sys
import numpy
import pandas

class DecisionTree:
  def __init__(self, feature):
    self.feature =  feature
    self.children = {}
  
  def add_branch(self, branch, child):
    self.children[branch] = child
  
  def get_branch_keys(self):
    return self.children.keys()

  def __str__(self, level=0):
    ret = "\t"*level+repr(self.feature)+"\n"
    for child, value in self.children.items():
      
      ret +=  str(child) + value.__str__(level+1)
    return ret


def ID3(df, Attributes, split_funct, max_depth,depth=0):
  """
  construncts the desicion tree
  S is the set of examples
  Label is the target label ??
  Attributes is the set of measurred attributes
  """
  # return most common label if all have the same label or max depth is reached 
  if is_unique(df["label"]) or (int(depth) == int(max_depth)):
    return DecisionTree(df['label'].value_counts().idxmax())
  # 1. Create  Root Node
  # 2. A = Attribute that best splits S
  # NOTE: do steps 2 then 1. 
  gain = float('-inf')
  best_attribute = None
  for attribute in Attributes.keys():
    poss_gain = information_gain(df, attribute, split_funct)
    if gain < poss_gain:
      gain = poss_gain
      best_attribute = attribute
 
  root = DecisionTree(best_attribute)
  # 3. for each v that A can take
  all_values = df[best_attribute].unique()
  for value in Attributes[best_attribute]:
    # a. add new branch to the  tree  A=v
    root.add_branch(value,  None)
    # let  S_v be  subset S where  A = v
    attribute_df = df[df[best_attribute] == value]
      # do the where remove the column
    # c. if S_v is empty  
    if attribute_df.empty:
      # add leaf node with most common value of label
      root.add_branch(value, DecisionTree(df['label'].value_counts().idxmax()))
    else:
      #add to subtree
      new_attributes = {key:val for key, val in attributes.items() if key != best_attribute}
      root.add_branch(value, ID3(attribute_df, new_attributes, split_funct,max_depth, depth+1))
      
   
  #  4. return root
  return root
  
def information_gain(df, attribute, splitFunction):
  total_split_value, total_values = splitFunction(df)
  allAttributes= df.groupby(attribute)[attribute].count()
  total_attribute_split = 0
  for index, row in allAttributes.items():
    
    attribute_split, count = splitFunction(df.loc[df[attribute] == index])
    total_attribute_split += (count / total_values) * attribute_split
  return (total_split_value - total_attribute_split)

def entropy(df):
  """
  returns the entropy of a set
  I think this is ok works on test1
  """
  set_entropy = 0
  allLabels = df.groupby("label")["label"].count()
  total  = allLabels.sum()
  
  for index, row in allLabels.items():
    probOfLabel = row/total
    set_entropy -= probOfLabel * numpy.log2(probOfLabel)
  return set_entropy, total

def gini_index(df):
  """
  calculates  the gini index
  """
  allLabels = df.groupby("label")["label"].count()
  total  = allLabels.sum()
  squares = 0
  for index, row in allLabels.items():
    probOfLabel = row/total
    squares += probOfLabel * probOfLabel
  return (1-squares), total

def majority_error(df):
  """
  calculated the me
  """
  allLabels = df.groupby("label")["label"].count()
  total = allLabels.sum()
  max = allLabels.max()
  return (total - max)/total, total

def get_training_data(file_path, columns):
  train_df = pandas.read_csv(file_path, names=columns, index_col=False)
 
  for column in columns:
    #numeric take median
    # and  replace with -  for less then 
    # and replace with  + for greater
    if column.endswith('(num)'):
      #print(column)
      median = train_df[column].median()
      train_df[column] = train_df[column].apply(lambda x: "+" if x >= median else '-')
  #print(train_df)
  return train_df

def read_columns(file_path):
  with open(file_path, 'r') as file:
    columns = []
    for line in file:
      columnsInLine = line.strip().split(",")
      columns += columnsInLine
    return columns

def test_data(tree, test_file,  columns):
  correct, incorrect = 0, 0
  test_df = pandas.read_csv(test_file, names=columns, index_col=False)
  # also need to do  the median converstion here
  for column in columns:
    #numeric take median
    # and  replace with -  for less then 
    # and replace with  + for greater
    if column.endswith('(num)'):
      #print(column)
      median = test_df[column].median()
      test_df[column] = test_df[column].apply(lambda x: "+" if x >= median else '-')

  for index, row in test_df.iterrows():
    if process_row(row, tree):
      correct  +=  1
    else:
      incorrect += 1

  return correct, incorrect

def process_row(row, tree):
  feature = tree.feature

  if tree.children and row[feature] in tree.children.keys():
    return process_row(row, tree.children[row[feature]])
    
  else:
    #leaf node
    return feature == row['label']
  

def is_unique(s):
  a = s.to_numpy() # s.values (pandas<0.24)
  return (a[0] == a).all()



if __name__ == "__main__":
  # argv will likely  look  like Train.csv Test.csv [labels] split_function max depth
  # or probably Train.csv Test.csv [column names] split_function max depth
  # i'd just love some properly formated data but nope.
  # use a * to specify numeric. 
  if len(sys.argv) >= 2:
    training_file = sys.argv[1]
    test_file = sys.argv[2]
    columns = read_columns(sys.argv[3])
    print(columns)
    split_funct_str = sys.argv[4]
    max_depth = sys.argv[5]
    split_funct = None
    if split_funct_str == "Information_Gain":
      split_funct = entropy
    elif split_funct_str =="Majority_Error":
      split_funct = majority_error
    elif split_funct_str ==  "Gini_Index":
      split_funct = gini_index
    else:
      print("enter a valid function")
      exit(1)

    train_df =  get_training_data(sys.argv[1], columns)
    #todo make a df of attributes with unique values so we can  created better trees
    # attribute_df = train_df.apply(lambda col: col.unique())
    attributes = {}
    for a in columns[:-1]:
      #print(a)
      attributes[a] = train_df[a].unique().flatten()
    #print(attributes) 
    #tree = ID3(train_df, train_df.columns[:-1])
    tree = ID3(train_df, attributes, split_funct ,max_depth)
    

    # print(str(tree))
    correct,  incorrect = test_data(tree, test_file, columns)
    print("correct is ", correct)
    print("incorrect is ", incorrect)
    error = (incorrect)/(correct + incorrect)
    print("error is ",  error)
  else: 
    print("no data given")